{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tom10\\edu\\cs229-proj\\env\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import preprocess.prepare as prepare\n",
    "import preprocess.word_bag as word_bag\n",
    "import model.naive_bayes as naive_bayes\n",
    "import model.nn as nn\n",
    "import model.trivial as trivial\n",
    "import metrics.metrics as metrics\n",
    "from utils.cache import cached\n",
    "from utils.data import train_dev_split\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached object from .cs229_cache\\crates_data.pkl\n"
     ]
    }
   ],
   "source": [
    "# Import and preprocess data\n",
    "data = cached(lambda:prepare.CratesData(), \"crates_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32870"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Selection data\n",
    "crates = [crate for crate in data.id2crates.values() if crate.category_indices]\n",
    "len(crates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached object from .cs229_cache\\dictionary.pkl\n",
      "Loading cached object from .cs229_cache\\wb10000.pkl\n"
     ]
    }
   ],
   "source": [
    "# Generate word bag\n",
    "dic = cached(lambda:word_bag.create_dictionary(crates), \"dictionary.pkl\")\n",
    "X, y = cached(lambda: word_bag.transform_crate(crates, dic, len(data.categories)), \"wb10000.pkl\")\n",
    "X = X.astype(np.float32)\n",
    "y = y.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42912"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train and dev datasets\n",
    "X_train, y_train, X_dev, y_dev = train_dev_split(X, y, train_ratio=0.8, seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Model Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 0a: Always false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "pred_a0_train = trivial.AlwaysFalseModel(len(data.categories)).predict(X_train)\n",
    "pred_a0_dev = trivial.AlwaysFalseModel(len(data.categories)).predict(X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exact_match_ratio: 0.0\n",
      "hamming_score: 0.9775566032244919\n",
      "precision_micro: 1.0\n",
      "precision_macro: 1.0\n",
      "precision_weighted: 1.0\n",
      "precision_samples: 1.0\n",
      "recall_micro: 0.0\n",
      "recall_macro: 0.024096385542168676\n",
      "recall_weighted: 0.0\n",
      "recall_samples: 0.0\n",
      "f1_micro: 0.0\n",
      "f1_macro: 0.024096385542168676\n",
      "f1_weighted: 0.0\n",
      "f1_samples: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Evaluate performance for train dataset\n",
    "metric_a0_train = metrics.evaluate_performance(y_train, pred_a0_train)\n",
    "for k, v in metric_a0_train.items():\n",
    "    print(k + \": \" + str(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exact_match_ratio: 0.0\n",
      "hamming_score: 0.9778200760997303\n",
      "precision_micro: 1.0\n",
      "precision_macro: 1.0\n",
      "precision_weighted: 1.0\n",
      "precision_samples: 1.0\n",
      "recall_micro: 0.0\n",
      "recall_macro: 0.04819277108433735\n",
      "recall_weighted: 0.0\n",
      "recall_samples: 0.0\n",
      "f1_micro: 0.0\n",
      "f1_macro: 0.04819277108433735\n",
      "f1_weighted: 0.0\n",
      "f1_samples: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Evaluate performance for dev dataset\n",
    "metric_a0_dev = metrics.evaluate_performance(y_dev, pred_a0_dev)\n",
    "for k, v in metric_a0_dev.items():\n",
    "    print(k + \": \" + str(v))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 0b: Random Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "pred_randl_train = trivial.RandomLabelingModel(len(data.categories)).predict(X_train)\n",
    "pred_randl_dev = trivial.RandomLabelingModel(len(data.categories)).predict(X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exact_match_ratio: 0.0\n",
      "hamming_score: 0.5005007857517024\n",
      "precision_micro: 0.02247302035044414\n",
      "precision_macro: 0.02245827275803694\n",
      "precision_weighted: 0.05814838632246531\n",
      "precision_samples: 0.022478993639010892\n",
      "recall_micro: 0.5001659751037344\n",
      "recall_macro: 0.5159583284465401\n",
      "recall_weighted: 0.5001659751037344\n",
      "recall_samples: 0.5006460547504026\n",
      "f1_micro: 0.04301340020500413\n",
      "f1_macro: 0.04048267767100961\n",
      "f1_weighted: 0.09891511518339502\n",
      "f1_samples: 0.04250368941827535\n"
     ]
    }
   ],
   "source": [
    "# Evaluate performance for train dataset\n",
    "metric_randl_train = metrics.evaluate_performance(y_train, pred_randl_train)\n",
    "for k, v in metric_randl_train.items():\n",
    "    print(k + \": \" + str(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exact_match_ratio: 0.0\n",
      "hamming_score: 0.4997606751010845\n",
      "precision_micro: 0.022320481470315658\n",
      "precision_macro: 0.022329784307113885\n",
      "precision_weighted: 0.059215327591118014\n",
      "precision_samples: 0.022329664869564968\n",
      "recall_micro: 0.5035687295322865\n",
      "recall_macro: 0.5380338541300888\n",
      "recall_weighted: 0.5035687295322865\n",
      "recall_samples: 0.4998402638223321\n",
      "f1_micro: 0.042746252485868864\n",
      "f1_macro: 0.04021593100385741\n",
      "f1_weighted: 0.1002126393371743\n",
      "f1_samples: 0.04223117544189479\n"
     ]
    }
   ],
   "source": [
    "# Evaluate performance for dev dataset\n",
    "metric_randl_dev = metrics.evaluate_performance(y_dev, pred_randl_dev)\n",
    "for k, v in metric_randl_dev.items():\n",
    "    print(k + \": \" + str(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 0c: Random Assigning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "pred_randa_train = trivial.RandomAssigningModel(len(data.categories)).predict(X_train)\n",
    "pred_randa_dev = trivial.RandomAssigningModel(len(data.categories)).predict(X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exact_match_ratio: 0.005449275362318841\n",
      "hamming_score: 0.9660364355974622\n",
      "precision_micro: 0.02191304347826087\n",
      "precision_macro: 0.022194974997056856\n",
      "precision_weighted: 0.056692731508664246\n",
      "precision_samples: 0.02191304347826087\n",
      "recall_micro: 0.011763485477178422\n",
      "recall_macro: 0.03581430874503741\n",
      "recall_weighted: 0.011763485477178422\n",
      "recall_samples: 0.01186731078904992\n",
      "f1_micro: 0.015308808639891999\n",
      "f1_macro: 0.011841893949020075\n",
      "f1_weighted: 0.01718443573130769\n",
      "f1_samples: 0.014533977455716584\n"
     ]
    }
   ],
   "source": [
    "# Evaluate performance for train dataset\n",
    "metric_randa_train = metrics.evaluate_performance(y_train, pred_randa_train)\n",
    "for k, v in metric_randa_train.items():\n",
    "    print(k + \": \" + str(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exact_match_ratio: 0.005101252125521719\n",
      "hamming_score: 0.9662821947862558\n",
      "precision_micro: 0.021177925490802288\n",
      "precision_macro: 0.02107686675449831\n",
      "precision_weighted: 0.0568092820978618\n",
      "precision_samples: 0.021177925490802288\n",
      "recall_micro: 0.011503904609958855\n",
      "recall_macro: 0.058343311704787876\n",
      "recall_weighted: 0.011503904609958855\n",
      "recall_samples: 0.011068171278404699\n",
      "f1_micro: 0.014909130482098161\n",
      "f1_macro: 0.010855000299093892\n",
      "f1_weighted: 0.017067795792531208\n",
      "f1_samples: 0.013665172360488484\n"
     ]
    }
   ],
   "source": [
    "# Evaluate performance for dev dataset\n",
    "metric_randa_dev = metrics.evaluate_performance(y_dev, pred_randa_dev)\n",
    "for k, v in metric_randa_dev.items():\n",
    "    print(k + \": \" + str(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting model:   0%|          | 0/83 [00:00<?, ?it/s]\rFitting model:   1%|          | 1/83 [00:04<06:39,  4.87s/it]\rFitting model:   2%|▏         | 2/83 [00:08<05:41,  4.22s/it]\rFitting model:   4%|▎         | 3/83 [00:12<05:20,  4.01s/it]\rFitting model:   5%|▍         | 4/83 [00:16<05:21,  4.07s/it]\rFitting model:   6%|▌         | 5/83 [00:20<05:00,  3.86s/it]\rFitting model:   7%|▋         | 6/83 [00:24<05:00,  3.90s/it]\rFitting model:   8%|▊         | 7/83 [00:27<04:52,  3.85s/it]\rFitting model:  10%|▉         | 8/83 [00:31<04:42,  3.77s/it]\rFitting model:  11%|█         | 9/83 [00:34<04:32,  3.69s/it]\rFitting model:  12%|█▏        | 10/83 [00:38<04:20,  3.57s/it]\rFitting model:  13%|█▎        | 11/83 [00:41<04:13,  3.52s/it]\rFitting model:  14%|█▍        | 12/83 [00:45<04:08,  3.49s/it]\rFitting model:  16%|█▌        | 13/83 [00:48<04:02,  3.46s/it]\rFitting model:  17%|█▋        | 14/83 [00:51<03:57,  3.45s/it]\rFitting model:  18%|█▊        | 15/83 [00:55<03:52,  3.42s/it]\rFitting model:  19%|█▉        | 16/83 [00:58<03:51,  3.46s/it]\rFitting model:  20%|██        | 17/83 [01:02<03:49,  3.48s/it]\rFitting model:  22%|██▏       | 18/83 [01:05<03:47,  3.49s/it]\rFitting model:  23%|██▎       | 19/83 [01:09<03:47,  3.55s/it]\rFitting model:  24%|██▍       | 20/83 [01:13<03:45,  3.58s/it]\rFitting model:  25%|██▌       | 21/83 [01:16<03:43,  3.61s/it]\rFitting model:  27%|██▋       | 22/83 [01:20<03:38,  3.58s/it]\rFitting model:  28%|██▊       | 23/83 [01:23<03:35,  3.59s/it]\rFitting model:  29%|██▉       | 24/83 [01:27<03:30,  3.57s/it]\rFitting model:  30%|███       | 25/83 [01:31<03:34,  3.70s/it]\rFitting model:  31%|███▏      | 26/83 [01:35<03:30,  3.70s/it]\rFitting model:  33%|███▎      | 27/83 [01:39<03:31,  3.78s/it]\rFitting model:  34%|███▎      | 28/83 [01:43<03:34,  3.90s/it]\rFitting model:  35%|███▍      | 29/83 [01:47<03:40,  4.09s/it]\rFitting model:  36%|███▌      | 30/83 [01:51<03:37,  4.11s/it]\rFitting model:  37%|███▋      | 31/83 [01:55<03:23,  3.91s/it]\rFitting model:  39%|███▊      | 32/83 [01:59<03:17,  3.88s/it]\rFitting model:  40%|███▉      | 33/83 [02:03<03:16,  3.93s/it]\rFitting model:  41%|████      | 34/83 [02:07<03:21,  4.11s/it]\rFitting model:  42%|████▏     | 35/83 [02:12<03:26,  4.31s/it]\rFitting model:  43%|████▎     | 36/83 [02:16<03:23,  4.32s/it]\rFitting model:  45%|████▍     | 37/83 [02:20<03:11,  4.16s/it]\rFitting model:  46%|████▌     | 38/83 [02:24<03:03,  4.07s/it]\rFitting model:  47%|████▋     | 39/83 [02:28<03:03,  4.16s/it]\rFitting model:  48%|████▊     | 40/83 [02:33<02:58,  4.15s/it]\rFitting model:  49%|████▉     | 41/83 [02:36<02:47,  3.99s/it]\rFitting model:  51%|█████     | 42/83 [02:40<02:44,  4.00s/it]\rFitting model:  52%|█████▏    | 43/83 [02:44<02:41,  4.04s/it]\rFitting model:  53%|█████▎    | 44/83 [02:49<02:42,  4.16s/it]\rFitting model:  54%|█████▍    | 45/83 [02:53<02:40,  4.22s/it]\rFitting model:  55%|█████▌    | 46/83 [02:57<02:27,  3.99s/it]\rFitting model:  57%|█████▋    | 47/83 [03:00<02:13,  3.72s/it]\rFitting model:  58%|█████▊    | 48/83 [03:03<02:04,  3.55s/it]\rFitting model:  59%|█████▉    | 49/83 [03:07<02:05,  3.68s/it]\rFitting model:  60%|██████    | 50/83 [03:11<02:09,  3.91s/it]\rFitting model:  61%|██████▏   | 51/83 [03:16<02:11,  4.12s/it]\rFitting model:  63%|██████▎   | 52/83 [03:20<02:10,  4.23s/it]\rFitting model:  64%|██████▍   | 53/83 [03:25<02:07,  4.25s/it]\rFitting model:  65%|██████▌   | 54/83 [03:28<01:58,  4.10s/it]\rFitting model:  66%|██████▋   | 55/83 [03:32<01:52,  4.00s/it]\rFitting model:  67%|██████▋   | 56/83 [03:36<01:44,  3.87s/it]\rFitting model:  69%|██████▊   | 57/83 [03:40<01:41,  3.90s/it]\rFitting model:  70%|██████▉   | 58/83 [03:44<01:41,  4.07s/it]\rFitting model:  71%|███████   | 59/83 [03:48<01:37,  4.07s/it]\rFitting model:  72%|███████▏  | 60/83 [03:52<01:30,  3.93s/it]\rFitting model:  73%|███████▎  | 61/83 [03:56<01:25,  3.86s/it]\rFitting model:  75%|███████▍  | 62/83 [03:59<01:21,  3.86s/it]\rFitting model:  76%|███████▌  | 63/83 [04:04<01:21,  4.06s/it]\rFitting model:  77%|███████▋  | 64/83 [04:08<01:14,  3.93s/it]\rFitting model:  78%|███████▊  | 65/83 [04:11<01:10,  3.91s/it]\rFitting model:  80%|███████▉  | 66/83 [04:16<01:08,  4.01s/it]\rFitting model:  81%|████████  | 67/83 [04:21<01:09,  4.34s/it]\rFitting model:  82%|████████▏ | 68/83 [04:24<01:01,  4.07s/it]\rFitting model:  83%|████████▎ | 69/83 [04:28<00:56,  4.04s/it]\rFitting model:  84%|████████▍ | 70/83 [04:32<00:50,  3.86s/it]\rFitting model:  86%|████████▌ | 71/83 [04:36<00:47,  3.96s/it]\rFitting model:  87%|████████▋ | 72/83 [04:40<00:43,  3.91s/it]\rFitting model:  88%|████████▊ | 73/83 [04:43<00:37,  3.78s/it]\rFitting model:  89%|████████▉ | 74/83 [04:47<00:33,  3.77s/it]\rFitting model:  90%|█████████ | 75/83 [04:50<00:29,  3.68s/it]\rFitting model:  92%|█████████▏| 76/83 [04:54<00:25,  3.71s/it]\rFitting model:  93%|█████████▎| 77/83 [04:58<00:23,  3.84s/it]\rFitting model:  94%|█████████▍| 78/83 [05:02<00:18,  3.70s/it]\rFitting model:  95%|█████████▌| 79/83 [05:06<00:15,  3.84s/it]\rFitting model:  96%|█████████▋| 80/83 [05:10<00:11,  3.89s/it]\rFitting model:  98%|█████████▊| 81/83 [05:14<00:07,  3.89s/it]\rFitting model:  99%|█████████▉| 82/83 [05:18<00:03,  3.96s/it]\rFitting model: 100%|██████████| 83/83 [05:22<00:00,  4.09s/it]\rFitting model: 100%|██████████| 83/83 [05:22<00:00,  3.89s/it]\n"
     ]
    }
   ],
   "source": [
    "# Fit model\n",
    "model_nb = naive_bayes.NaiveBayesModel(num_categories=len(data.categories))\n",
    "model_nb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting categories:   0%|          | 0/83 [00:00<?, ?it/s]\rPredicting categories:   1%|          | 1/83 [00:00<00:20,  4.05it/s]\rPredicting categories:   2%|▏         | 2/83 [00:00<00:14,  5.43it/s]\rPredicting categories:   4%|▎         | 3/83 [00:00<00:12,  6.42it/s]\rPredicting categories:   5%|▍         | 4/83 [00:00<00:10,  7.43it/s]\rPredicting categories:   8%|▊         | 7/83 [00:00<00:06, 11.49it/s]\rPredicting categories:  11%|█         | 9/83 [00:00<00:06, 11.37it/s]\rPredicting categories:  13%|█▎        | 11/83 [00:01<00:06, 11.49it/s]\rPredicting categories:  16%|█▌        | 13/83 [00:01<00:05, 11.68it/s]\rPredicting categories:  18%|█▊        | 15/83 [00:01<00:06, 10.70it/s]\rPredicting categories:  20%|██        | 17/83 [00:01<00:06, 10.49it/s]\rPredicting categories:  23%|██▎       | 19/83 [00:01<00:06, 10.54it/s]\rPredicting categories:  27%|██▋       | 22/83 [00:02<00:04, 12.44it/s]\rPredicting categories:  29%|██▉       | 24/83 [00:02<00:05, 11.67it/s]\rPredicting categories:  31%|███▏      | 26/83 [00:02<00:04, 11.79it/s]\rPredicting categories:  34%|███▎      | 28/83 [00:02<00:04, 11.67it/s]\rPredicting categories:  36%|███▌      | 30/83 [00:02<00:04, 11.67it/s]\rPredicting categories:  39%|███▊      | 32/83 [00:02<00:04, 11.43it/s]\rPredicting categories:  41%|████      | 34/83 [00:03<00:04, 11.21it/s]\rPredicting categories:  43%|████▎     | 36/83 [00:03<00:04, 11.44it/s]\rPredicting categories:  46%|████▌     | 38/83 [00:03<00:03, 11.35it/s]\rPredicting categories:  48%|████▊     | 40/83 [00:03<00:03, 11.08it/s]\rPredicting categories:  51%|█████     | 42/83 [00:03<00:03, 11.10it/s]\rPredicting categories:  53%|█████▎    | 44/83 [00:04<00:03, 11.06it/s]\rPredicting categories:  55%|█████▌    | 46/83 [00:04<00:03, 11.14it/s]\rPredicting categories:  58%|█████▊    | 48/83 [00:04<00:03, 11.33it/s]\rPredicting categories:  60%|██████    | 50/83 [00:04<00:02, 11.20it/s]\rPredicting categories:  63%|██████▎   | 52/83 [00:04<00:02, 10.94it/s]\rPredicting categories:  65%|██████▌   | 54/83 [00:04<00:02, 10.99it/s]\rPredicting categories:  67%|██████▋   | 56/83 [00:05<00:02, 11.03it/s]\rPredicting categories:  70%|██████▉   | 58/83 [00:05<00:02, 11.26it/s]\rPredicting categories:  72%|███████▏  | 60/83 [00:05<00:01, 11.51it/s]\rPredicting categories:  75%|███████▍  | 62/83 [00:05<00:01, 11.37it/s]\rPredicting categories:  77%|███████▋  | 64/83 [00:05<00:01, 11.31it/s]\rPredicting categories:  80%|███████▉  | 66/83 [00:06<00:01, 11.29it/s]\rPredicting categories:  82%|████████▏ | 68/83 [00:06<00:01, 11.28it/s]\rPredicting categories:  84%|████████▍ | 70/83 [00:06<00:01, 11.24it/s]\rPredicting categories:  87%|████████▋ | 72/83 [00:06<00:00, 11.25it/s]\rPredicting categories:  89%|████████▉ | 74/83 [00:06<00:00, 11.05it/s]\rPredicting categories:  92%|█████████▏| 76/83 [00:06<00:00, 10.72it/s]\rPredicting categories:  94%|█████████▍| 78/83 [00:07<00:00, 10.76it/s]\rPredicting categories:  96%|█████████▋| 80/83 [00:07<00:00, 10.92it/s]\rPredicting categories:  99%|█████████▉| 82/83 [00:07<00:00, 10.49it/s]\rPredicting categories: 100%|██████████| 83/83 [00:07<00:00, 10.91it/s]\n",
      "\rPredicting categories:   0%|          | 0/83 [00:00<?, ?it/s]\rPredicting categories:   4%|▎         | 3/83 [00:00<00:03, 24.40it/s]\rPredicting categories:  10%|▉         | 8/83 [00:00<00:02, 36.22it/s]\rPredicting categories:  16%|█▌        | 13/83 [00:00<00:01, 38.06it/s]\rPredicting categories:  20%|██        | 17/83 [00:00<00:01, 36.75it/s]\rPredicting categories:  27%|██▋       | 22/83 [00:00<00:01, 38.09it/s]\rPredicting categories:  31%|███▏      | 26/83 [00:00<00:01, 36.48it/s]\rPredicting categories:  36%|███▌      | 30/83 [00:00<00:01, 35.53it/s]\rPredicting categories:  41%|████      | 34/83 [00:00<00:01, 36.62it/s]\rPredicting categories:  46%|████▌     | 38/83 [00:01<00:01, 35.11it/s]\rPredicting categories:  51%|█████     | 42/83 [00:01<00:01, 36.27it/s]\rPredicting categories:  55%|█████▌    | 46/83 [00:01<00:01, 34.17it/s]\rPredicting categories:  60%|██████    | 50/83 [00:01<00:00, 33.27it/s]\rPredicting categories:  65%|██████▌   | 54/83 [00:01<00:00, 33.93it/s]\rPredicting categories:  70%|██████▉   | 58/83 [00:01<00:00, 35.18it/s]\rPredicting categories:  75%|███████▍  | 62/83 [00:01<00:00, 33.96it/s]\rPredicting categories:  80%|███████▉  | 66/83 [00:01<00:00, 33.74it/s]\rPredicting categories:  84%|████████▍ | 70/83 [00:01<00:00, 34.46it/s]\rPredicting categories:  89%|████████▉ | 74/83 [00:02<00:00, 34.94it/s]\rPredicting categories:  94%|█████████▍| 78/83 [00:02<00:00, 33.44it/s]\rPredicting categories:  99%|█████████▉| 82/83 [00:02<00:00, 33.55it/s]\rPredicting categories: 100%|██████████| 83/83 [00:02<00:00, 34.79it/s]\n"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "pred_nb_train = model_nb.predict(X_train)\n",
    "pred_nb_dev = model_nb.predict(X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [25875, 3]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [87], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Evaluate performance for train dataset\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m metric_nb_train \u001b[39m=\u001b[39m metrics\u001b[39m.\u001b[39;49mevaluate_performance(y_train, pred_nb_train)\n\u001b[1;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m metric_nb_train\u001b[39m.\u001b[39mitems():\n\u001b[1;32m      4\u001b[0m     \u001b[39mprint\u001b[39m(k \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(v))\n",
      "File \u001b[0;32m~/Documents/Stanford Coursework/CS229/cs229-proj-main/metrics/metrics.py:12\u001b[0m, in \u001b[0;36mevaluate_performance\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     10\u001b[0m results \u001b[39m=\u001b[39m {}\n\u001b[1;32m     11\u001b[0m \u001b[39mfor\u001b[39;00m m \u001b[39min\u001b[39;00m metrics:\n\u001b[0;32m---> 12\u001b[0m     k, v \u001b[39m=\u001b[39m m(y_true, y_pred)\n\u001b[1;32m     13\u001b[0m     results[k] \u001b[39m=\u001b[39m v\n\u001b[1;32m     14\u001b[0m \u001b[39mreturn\u001b[39;00m results\n",
      "File \u001b[0;32m~/Documents/Stanford Coursework/CS229/cs229-proj-main/metrics/metrics.py:19\u001b[0m, in \u001b[0;36maccuracy_emr\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39maccuracy_emr\u001b[39m(y_true, y_pred):\n\u001b[1;32m     18\u001b[0m     label \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mexact_match_ratio\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 19\u001b[0m     \u001b[39mreturn\u001b[39;00m label, sklearn\u001b[39m.\u001b[39;49maccuracy_score(y_true, y_pred)\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:211\u001b[0m, in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[39m\"\"\"Accuracy classification score.\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \n\u001b[1;32m    147\u001b[0m \u001b[39mIn multilabel classification, this function computes subset accuracy:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[39m0.5\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    210\u001b[0m \u001b[39m# Compute accuracy for each possible representation\u001b[39;00m\n\u001b[0;32m--> 211\u001b[0m y_type, y_true, y_pred \u001b[39m=\u001b[39m _check_targets(y_true, y_pred)\n\u001b[1;32m    212\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[1;32m    213\u001b[0m \u001b[39mif\u001b[39;00m y_type\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39mmultilabel\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.9/site-packages/sklearn/metrics/_classification.py:84\u001b[0m, in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_check_targets\u001b[39m(y_true, y_pred):\n\u001b[1;32m     58\u001b[0m     \u001b[39m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \n\u001b[1;32m     60\u001b[0m \u001b[39m    This converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[39m    y_pred : array or indicator matrix\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 84\u001b[0m     check_consistent_length(y_true, y_pred)\n\u001b[1;32m     85\u001b[0m     type_true \u001b[39m=\u001b[39m type_of_target(y_true, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my_true\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     86\u001b[0m     type_pred \u001b[39m=\u001b[39m type_of_target(y_pred, input_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39my_pred\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.9/site-packages/sklearn/utils/validation.py:387\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    385\u001b[0m uniques \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(lengths)\n\u001b[1;32m    386\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(uniques) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 387\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    388\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    389\u001b[0m         \u001b[39m%\u001b[39m [\u001b[39mint\u001b[39m(l) \u001b[39mfor\u001b[39;00m l \u001b[39min\u001b[39;00m lengths]\n\u001b[1;32m    390\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [25875, 3]"
     ]
    }
   ],
   "source": [
    "# Evaluate performance for train dataset\n",
    "metric_nb_train = metrics.evaluate_performance(y_train, pred_nb_train)\n",
    "for k, v in metric_nb_train.items():\n",
    "    print(k + \": \" + str(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate performance for dev dataset\n",
    "metric_nb_dev = metrics.evaluate_performance(y_dev, pred_nb_dev)\n",
    "for k, v in metric_nb_dev.items():\n",
    "    print(k + \": \" + str(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.0050, val_loss: 0.0040, Hamming Distance: 0.0202, val_hamming_dist: 0.0204\n",
      "Epoch: 1, Loss: 0.0035, val_loss: 0.0034, Hamming Distance: 0.0194, val_hamming_dist: 0.0225\n",
      "Epoch: 2, Loss: 0.0029, val_loss: 0.0032, Hamming Distance: 0.0168, val_hamming_dist: 0.0193\n",
      "Epoch: 3, Loss: 0.0025, val_loss: 0.0029, Hamming Distance: 0.0158, val_hamming_dist: 0.0212\n",
      "Epoch: 4, Loss: 0.0023, val_loss: 0.0028, Hamming Distance: 0.0146, val_hamming_dist: 0.0182\n",
      "Epoch: 5, Loss: 0.0020, val_loss: 0.0025, Hamming Distance: 0.0128, val_hamming_dist: 0.0192\n",
      "Epoch: 6, Loss: 0.0018, val_loss: 0.0024, Hamming Distance: 0.0122, val_hamming_dist: 0.0175\n",
      "Epoch: 7, Loss: 0.0016, val_loss: 0.0022, Hamming Distance: 0.0099, val_hamming_dist: 0.0176\n",
      "Epoch: 8, Loss: 0.0015, val_loss: 0.0022, Hamming Distance: 0.0098, val_hamming_dist: 0.0170\n",
      "Epoch: 9, Loss: 0.0013, val_loss: 0.0021, Hamming Distance: 0.0084, val_hamming_dist: 0.0169\n",
      "Epoch: 10, Loss: 0.0012, val_loss: 0.0020, Hamming Distance: 0.0084, val_hamming_dist: 0.0167\n",
      "Epoch: 11, Loss: 0.0011, val_loss: 0.0019, Hamming Distance: 0.0074, val_hamming_dist: 0.0166\n",
      "Epoch: 12, Loss: 0.0010, val_loss: 0.0019, Hamming Distance: 0.0073, val_hamming_dist: 0.0167\n",
      "Epoch: 13, Loss: 0.0009, val_loss: 0.0019, Hamming Distance: 0.0067, val_hamming_dist: 0.0167\n",
      "Epoch: 14, Loss: 0.0009, val_loss: 0.0019, Hamming Distance: 0.0067, val_hamming_dist: 0.0168\n",
      "Epoch: 15, Loss: 0.0008, val_loss: 0.0018, Hamming Distance: 0.0063, val_hamming_dist: 0.0167\n",
      "Epoch: 16, Loss: 0.0008, val_loss: 0.0018, Hamming Distance: 0.0061, val_hamming_dist: 0.0167\n",
      "Epoch: 17, Loss: 0.0007, val_loss: 0.0018, Hamming Distance: 0.0059, val_hamming_dist: 0.0168\n",
      "Epoch: 18, Loss: 0.0007, val_loss: 0.0018, Hamming Distance: 0.0057, val_hamming_dist: 0.0166\n",
      "Epoch: 19, Loss: 0.0006, val_loss: 0.0018, Hamming Distance: 0.0054, val_hamming_dist: 0.0167\n"
     ]
    }
   ],
   "source": [
    "# Fit model\n",
    "model_lr = nn.DNNModel(num_categories=len(data.categories), learning_rate=0.0001, reg=0.000)\n",
    "model_lr.fit(nn.LogisticRegression, X_train, y_train, X_dev, y_dev, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "pred_lr_train = model_lr.predict(X_train)\n",
    "pred_lr_dev = model_lr.predict(X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exact_match_ratio: 0.7102222222222222\n",
      "hamming_score: 0.9945740061696059\n",
      "precision_micro: 0.9738855320142112\n",
      "precision_macro: 0.9817735535727087\n",
      "precision_weighted: 0.9742572750838704\n",
      "precision_samples: 0.9777726247987115\n",
      "recall_micro: 0.7791286307053942\n",
      "recall_macro: 0.7426233315575526\n",
      "recall_weighted: 0.7791286307053942\n",
      "recall_samples: 0.7781745571658615\n",
      "f1_micro: 0.8656885005935846\n",
      "f1_macro: 0.8415025022384366\n",
      "f1_weighted: 0.8644996397455661\n",
      "f1_samples: 0.787699638203986\n"
     ]
    }
   ],
   "source": [
    "# Evaluate performance for train dataset\n",
    "metric_lr_train = metrics.evaluate_performance(y_train, pred_lr_train)\n",
    "for k, v in metric_lr_train.items():\n",
    "    print(k + \": \" + str(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exact_match_ratio: 0.2731488638120266\n",
      "hamming_score: 0.9832807811862692\n",
      "precision_micro: 0.7268647477561127\n",
      "precision_macro: 0.7246169115142638\n",
      "precision_weighted: 0.71787201141928\n",
      "precision_samples: 0.8530813010523941\n",
      "recall_micro: 0.39440759089764044\n",
      "recall_macro: 0.31646299059637334\n",
      "recall_weighted: 0.39440759089764044\n",
      "recall_samples: 0.4136780543103004\n",
      "f1_micro: 0.5113494093952424\n",
      "f1_macro: 0.41134598122975663\n",
      "f1_weighted: 0.4990349950199903\n",
      "f1_samples: 0.41422893045109405\n"
     ]
    }
   ],
   "source": [
    "# Evaluate performance for dev dataset\n",
    "metric_lr_dev = metrics.evaluate_performance(y_dev, pred_lr_dev)\n",
    "for k, v in metric_lr_dev.items():\n",
    "    print(k + \": \" + str(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3: Nerual Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported format string passed to tuple.__format__",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [93], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Fit model\u001b[39;00m\n\u001b[1;32m      2\u001b[0m model_nn \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mDNNModel(num_categories\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(data\u001b[39m.\u001b[39mcategories), learning_rate\u001b[39m=\u001b[39m\u001b[39m0.0001\u001b[39m, reg\u001b[39m=\u001b[39m\u001b[39m0.000\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m model_nn\u001b[39m.\u001b[39;49mfit(nn\u001b[39m.\u001b[39;49mMyNet, X_train, y_train, X_dev, y_dev, epochs\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m)\n",
      "File \u001b[0;32m~/Documents/Stanford Coursework/CS229/cs229-proj-main/model/nn.py:105\u001b[0m, in \u001b[0;36mDNNModel.fit\u001b[0;34m(self, model_func, X_train, y_train, X_val, y_val, epochs, batch_size)\u001b[0m\n\u001b[1;32m    103\u001b[0m train_hamming_dist \u001b[39m=\u001b[39m hamming_distance(y_train, y_train_pred)\n\u001b[1;32m    104\u001b[0m val_hamming_dist \u001b[39m=\u001b[39m hamming_distance(y_val, y_val_pred)\n\u001b[0;32m--> 105\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39;49m\u001b[39mEpoch: \u001b[39;49m\u001b[39m{}\u001b[39;49;00m\u001b[39m, Loss: \u001b[39;49m\u001b[39m{:.4f}\u001b[39;49;00m\u001b[39m, val_loss: \u001b[39;49m\u001b[39m{:.4f}\u001b[39;49;00m\u001b[39m, Hamming Distance: \u001b[39;49m\u001b[39m{:.4f}\u001b[39;49;00m\u001b[39m, val_hamming_dist: \u001b[39;49m\u001b[39m{:.4f}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49mformat(epoch, train_loss, val_loss, train_hamming_dist, val_hamming_dist))\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported format string passed to tuple.__format__"
     ]
    }
   ],
   "source": [
    "# Fit model\n",
    "model_nn = nn.DNNModel(num_categories=len(data.categories), learning_rate=0.0001, reg=0.000)\n",
    "model_nn.fit(nn.MyNet, X_train, y_train, X_dev, y_dev, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "pred_nn_train = model_nn.predict(X_train)\n",
    "pred_nn_dev = model_nn.predict(X_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exact_match_ratio: 0.7337971014492753\n",
      "hamming_score: 0.9946908794598684\n",
      "precision_micro: 0.8958306441203047\n",
      "precision_macro: 0.9063990462515596\n",
      "precision_weighted: 0.9067004409559278\n",
      "precision_samples: 0.9224912353347136\n",
      "recall_micro: 0.8639004149377594\n",
      "recall_macro: 0.7477845210964775\n",
      "recall_weighted: 0.8639004149377594\n",
      "recall_samples: 0.894112077294686\n",
      "f1_micro: 0.8795758433493166\n",
      "f1_macro: 0.7933051891740572\n",
      "f1_weighted: 0.875876399563169\n",
      "f1_samples: 0.8851302078490484\n"
     ]
    }
   ],
   "source": [
    "# Evaluate performance for train dataset\n",
    "metric_nn_train = metrics.evaluate_performance(y_train, pred_nn_train)\n",
    "for k, v in metric_nn_train.items():\n",
    "    print(k + \": \" + str(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exact_match_ratio: 0.30514762714484467\n",
      "hamming_score: 0.9817908207260948\n",
      "precision_micro: 0.6009852216748769\n",
      "precision_macro: 0.6604618164519468\n",
      "precision_weighted: 0.6310058768211979\n",
      "precision_samples: 0.6636184292854567\n",
      "recall_micro: 0.5327063565370728\n",
      "recall_macro: 0.41715158266001395\n",
      "recall_weighted: 0.5327063565370728\n",
      "recall_samples: 0.5693048899881487\n",
      "f1_micro: 0.5647896728243935\n",
      "f1_macro: 0.46691600584627413\n",
      "f1_weighted: 0.5512077467143873\n",
      "f1_samples: 0.5392691350490083\n"
     ]
    }
   ],
   "source": [
    "# Evaluate performance for dev dataset\n",
    "metric_nn_dev = metrics.evaluate_performance(y_dev, pred_nn_dev)\n",
    "for k, v in metric_nn_dev.items():\n",
    "    print(k + \": \" + str(v))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "86d31b2c839410b1471e32d2b416a55b336f222b5dfb4e49f62c4dd209883d8c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
