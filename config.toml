[models.logistic]
name = "logistic"
batch_size = 128
learning_rate = 1e-3
seq_len = 256
weight_decay = 5e-1
num_categories = 83
pos_weight_threshold = 4.0 # note: pos_weight_threshold = 1.0 means no weighting in logistic regression
loss = "weighted_bce"

[models.nn]
name = "nn"
batch_size = 128
learning_rate = 2e-4
seq_len = 256
weight_decay = 5e-1
num_categories = 83
pos_weight_threshold = 4.0
loss = "weighted_bce"      # choose from "weighted_bce", "asymmetric"

[models.lstm]
name = "lstm"
batch_size = 128
learning_rate = 5e-4
seq_len = 64
num_categories = 83
pos_weight_threshold = 2.0
weight_decay = 4e-1
hidden_size = 128
dropout_p = 0.5

[models.distil_bert]
name = "distil_bert"
pretrained = "distilbert-base-uncased"
batch_size = 32
learning_rate = 5e-5
max_length = 256                       # 512 is the max length for distilbert
dropout_p = 0.6
dropout_p_from_depnet = 0.3
num_categories = 83
pos_weight_threshold = 2.0
weight_decay = 1e-1
no_dep = false
loss = "assymetric"

[models.distil_bert.depnet]
name = "depnet"
hidden_dim = 512
learning_rate = 1e-4
num_categories = 83
weight_decay = 4e-1
dropout_p = 0.5
n_epochs = 50
